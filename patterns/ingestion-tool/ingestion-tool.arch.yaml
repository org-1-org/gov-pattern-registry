# ingestion-tool.arch.yaml
# Pattern: Generic data ingestion tool architecture
# Version: 0.1.0
#
# This pattern captures the stable invariants of a data ingestion tool that:
# - Connects to external data sources via adapters
# - Handles authentication/credential management
# - Streams raw records with optional time-window filtering
# - Delegates state management, event wrapping, and persistence to consumers

kind: arch
version: "0.1.0"
level: project

metadata:
  name: ingestion-tool
  description: |
    Reusable architecture pattern for thin data ingestion layers.
    Covers auth, adapters, and raw record streaming.
    Does NOT cover state management, event envelopes, or persistence.
  tags:
    - ingestion
    - etl
    - data-pipeline
    - adapters

# -----------------------------------------------------------------------------
# Containers (conceptual components, not deployment units)
# -----------------------------------------------------------------------------
containers:
  entrypoint:
    description: |
      Programmatic API entry point for stream resolution and instantiation.
      May be a Python module, function, or class. CLI is a separate optional surface.
    required: true
    responsibilities:
      - Stream identifier resolution
      - Credential lookup orchestration
      - Adapter instantiation
    boundaries:
      exposes:
        - get_stream(stream_id, **options) -> SourceStream
        - Stream protocol/interface definition

  cli:
    description: |
      Optional command-line interface for interactive use or scripting.
      Not all ingestion tools expose a CLI; library-only tools skip this.
    required: false
    responsibilities:
      - Parse command-line arguments
      - Invoke entrypoint API
      - Format output for terminal
    boundaries:
      exposes:
        - CLI commands (e.g., `ingest extract --stream <name>`)
      depends_on:
        - entrypoint

  credential_store:
    description: |
      Abstraction over secure credential storage.
      May be backed by files, vaults, environment variables, or external services.
    required: true
    responsibilities:
      - Load credentials by identity
      - Support multiple credential types (OAuth2, API keys, username/password)
      - Isolate secrets from adapter code
    boundaries:
      exposes:
        - load(identity) -> Credentials
      depends_on:
        - External credential backend (vault, authctl, env, etc.)

  auth_manager:
    description: |
      Per-provider authentication handler.
      Builds SDK clients, manages token refresh, handles auth flows.
    required: true
    responsibilities:
      - Build authenticated SDK clients
      - Handle token refresh (delegate to provider SDK where possible)
      - Support provider-specific auth flows (OAuth2, device code, API key)
    boundaries:
      depends_on:
        - credential_store
      exposes:
        - get_client(identity) -> ProviderClient

  connector_adapter:
    description: |
      Source-specific adapter that knows how to extract records from a single source.
      Handles pagination, rate limiting, and provider-specific date filtering.
    required: true
    responsibilities:
      - Implement extract(since, until) -> Iterator[RawRecord]
      - Handle pagination transparently
      - Apply provider-specific date/time filtering
      - Yield raw provider records unchanged
    boundaries:
      depends_on:
        - auth_manager
      exposes:
        - source: str (provider name)
        - stream: str (object type)
        - extract(since, until) -> Iterator[RawRecord]

  normalizer:
    description: |
      Optional component for record transformation before emission.
      Most thin ingestion tools skip this and pass raw records through.
    required: false
    responsibilities:
      - Transform provider-specific records to canonical format (if needed)
      - Field mapping and type coercion
    boundaries:
      depends_on:
        - connector_adapter output
      exposes:
        - normalize(raw_record) -> CanonicalRecord

  emitter:
    description: |
      Output stage that delivers records to consumers.
      In thin ingestion tools, this is often just yielding to the caller.
      Event wrapping and persistence typically live in the consumer.
    required: false
    responsibilities:
      - Deliver records to downstream consumers
      - May implement batching or buffering
    boundaries:
      depends_on:
        - connector_adapter (or normalizer if present)
      exposes:
        - Iterator[RawRecord] or push to downstream

# -----------------------------------------------------------------------------
# Boundaries (interfaces at system edges)
# -----------------------------------------------------------------------------
boundaries:
  input:
    description: External APIs, files, or data sources
    examples:
      - REST APIs (Gmail, Stripe, MS Graph)
      - OData endpoints (Dataverse)
      - GraphQL APIs
      - File systems (PDFs, CSVs)
    contracts:
      - Adapters declare their source type and version
      - Adapters handle provider-specific pagination

  output:
    description: Raw records emitted to consumers
    contracts:
      - Records are raw provider dicts (no envelope yet)
      - Caller receives Iterator[dict] or similar
      - Envelope wrapping is consumer responsibility

  storage:
    description: Token/session cache location
    contracts:
      - Credentials loaded from external store (not embedded)
      - Token caches are local-only, never committed to version control
      - Token cache path must be predictable and documented

# -----------------------------------------------------------------------------
# Architectural Decisions (ADR-style, with IDs for reference)
# -----------------------------------------------------------------------------
decisions:
  ingest.cursor.model:
    id: ADR-001
    title: Cursor/checkpoint ownership
    status: accepted
    context: |
      Ingestion tools can either manage their own cursors (remembering last-seen)
      or delegate cursor management to the consumer.
    decision: |
      Cursors and watermarks are managed by the CONSUMER, not the ingestion tool.
      The ingestion tool accepts (since, until) parameters and filters accordingly.
      This keeps the ingestion layer stateless.
    consequences:
      - Simpler ingestion tool (no state to manage)
      - Consumer has full control over replay and backfill
      - Requires consumer to track progress

  ingest.idempotency:
    id: ADR-002
    title: Idempotency strategy
    status: accepted
    context: |
      Duplicate records can occur due to pagination edge cases, retries, or
      overlapping time windows.
    decision: |
      The ingestion tool does NOT deduplicate. It yields all records matching
      the time window. The consumer is responsible for idempotency (via
      external_id or composite keys).
    consequences:
      - Ingestion tool remains simple and stateless
      - Consumer must implement deduplication logic
      - Records have provider-assigned IDs for dedup keys

  ingest.output.envelope:
    id: ADR-003
    title: Output envelope format
    status: accepted
    context: |
      Records need metadata (source, type, timestamps) for downstream processing.
    decision: |
      Ingestion tool yields RAW provider records. Envelope wrapping (adding
      source_system, object_type, idem_key, event type) is done by the consumer.
    consequences:
      - Ingestion tool is reusable across different event schemas
      - Consumer controls envelope format
      - Adapters expose source/stream metadata for envelope construction

  ingest.failure.policy:
    id: ADR-004
    title: Error handling and retries
    status: accepted
    context: |
      API calls can fail due to rate limits, network issues, or auth expiry.
    decision: |
      - Auth errors: Attempt token refresh once, then fail
      - Rate limits: Respect provider backoff, retry with exponential backoff
      - Network errors: Retry with backoff, fail after N attempts
      - Parse errors: Log and skip individual records, continue stream
    consequences:
      - Partial failures don't abort entire extraction
      - Consumer receives what was successfully extracted
      - Errors are logged for observability

  ingest.auth.isolation:
    id: ADR-005
    title: Credential isolation
    status: accepted
    context: |
      Secrets must not be embedded in code or passed through adapters.
    decision: |
      Credentials are loaded via a CredentialStore abstraction. Adapters
      receive authenticated clients, never raw secrets.
    consequences:
      - Secrets stay in secure storage
      - Adapters are testable with mock clients
      - Credential rotation doesn't require code changes

  ingest.token_cache.locality:
    id: ADR-006
    title: Token cache locality
    status: accepted
    context: |
      OAuth and session tokens need to be cached for refresh, but must not
      leak into version control or be shared across environments.
    decision: |
      Token caches are stored in a predictable local path (e.g., .tokens/ or
      ~/.local/share/<tool>/tokens/). This path must be:
      - Gitignored in all cases
      - Local to the execution environment
      - Never depended upon by adapters at import time
    consequences:
      - Tokens don't leak to version control
      - Each environment has isolated token state
      - Path is documented and predictable

# -----------------------------------------------------------------------------
# Observability expectations
# -----------------------------------------------------------------------------
observability:
  recommended:
    logging:
      - Stream start/end with identity (not credentials)
      - Record counts per extraction
      - Auth refresh events
      - Errors with context (provider, stream, error type)
    metrics:
      - records_extracted_total (by source, stream)
      - extraction_duration_seconds (by source, stream)
      - auth_refresh_total (by provider)
      - errors_total (by source, stream, error_type)
  not_required:
    - Distributed tracing (consumer responsibility)
    - Alerting rules (deployment concern)
